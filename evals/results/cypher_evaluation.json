{
  "timestamp": "2025-12-14T15:24:09.168742",
  "num_questions": 6,
  "summary": {
    "avg_hit_rate": 0.5,
    "avg_mrr": 0.5,
    "avg_judge_score": 0.44166666666666665,
    "avg_num_tokens": 9858.833333333334,
    "total_tokens": 59153,
    "avg_combined_score": 0.07870969357958486,
    "best_combined_score": 0.2192270333226254
  },
  "results": [
    {
      "question": "What questions are tagged with user-behavior?",
      "hit_rate": 1.0,
      "mrr": 1.0,
      "judge_score": 0.75,
      "num_tokens": 8778,
      "combined_score": 0.2192270333226254,
      "query_used": "MATCH (q:Question)-[:HAS_TAG]->(t:Tag {name: 'user-behavior'}) RETURN q.question_id, q.title"
    },
    {
      "question": "Which questions have the most answers?",
      "hit_rate": 1.0,
      "mrr": 1.0,
      "judge_score": 0.75,
      "num_tokens": 25410,
      "combined_score": 0.12885152488950927,
      "query_used": "MATCH (q:Question)-[:HAS_ANSWER]->(a:Answer) \nWITH q, count(a) as answer_count \nORDER BY answer_count DESC \nRETURN q.question_id, answer_count"
    },
    {
      "question": "What questions have accepted answers?",
      "hit_rate": 1.0,
      "mrr": 1.0,
      "judge_score": 0.65,
      "num_tokens": 17809,
      "combined_score": 0.12417960326537457,
      "query_used": "MATCH (q:Question)-[:ACCEPTED]->(a:Answer) RETURN q.question_id, a.answer_id"
    },
    {
      "question": "Which user has asked the most questions?",
      "hit_rate": 0.0,
      "mrr": 0.0,
      "judge_score": 0.5,
      "num_tokens": 7156,
      "combined_score": 0.0,
      "query_used": "MATCH (u:User)-[:ASKED]->(q:Question)\nWITH u, count(q) as question_count\nORDER BY question_count DESC\nLIMIT 1\nRETURN u.display_name, question_count"
    },
    {
      "question": "Which users have both asked questions and provided answers?",
      "hit_rate": 0.0,
      "mrr": 0.0,
      "judge_score": 0.0,
      "num_tokens": 0,
      "combined_score": 0.0,
      "query_used": ""
    },
    {
      "question": "What questions mention 'user' in their title or body?",
      "hit_rate": 0.0,
      "mrr": 0.0,
      "judge_score": 0.0,
      "num_tokens": 0,
      "combined_score": 0.0,
      "query_used": ""
    }
  ],
  "metadata": {
    "ground_truth_file": "evals/cypher_ground_truth.json",
    "total_questions_in_ground_truth": 6,
    "agent_type": "cypher",
    "judge_model": "gpt-4o-mini",
    "max_samples": 10,
    "sampled": true
  }
}
